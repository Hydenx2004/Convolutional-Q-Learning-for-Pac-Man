{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hydenx2004/Convolutional-Q-Learning-for-Pac-Man/blob/main/Deep_Convolutional_Q_Learning_for_Pac_Man.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAiHVEoWHy_D"
      },
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjO1aK3Ddjs5"
      },
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbnq3XpoKa_7",
        "outputId": "54caec95-d61d-430d-edb4-7c94a2934766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2024.6.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=4347f87e66ff1f2ca86883e6113415f88b501ddfecab316c11c10272878c4575\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,387 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121913 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349121 sha256=61c2e12223801de5f1f5cb21877d08954e3b99cc148f2dd019394e8cde2f0bb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wes4LZdxdd"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7wa0ft8e3M_"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYVpVdHe-i6"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-i8WXmL1Vof"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Neural Network for processing visual input and producing action values.\n",
        "\n",
        "    Attributes:\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        conv1 (nn.Conv2d): First convolutional layer.\n",
        "        bn1 (nn.BatchNorm2d): Batch normalization for the first convolutional layer.\n",
        "        conv2 (nn.Conv2d): Second convolutional layer.\n",
        "        bn2 (nn.BatchNorm2d): Batch normalization for the second convolutional layer.\n",
        "        conv3 (nn.Conv2d): Third convolutional layer.\n",
        "        bn3 (nn.BatchNorm2d): Batch normalization for the third convolutional layer.\n",
        "        conv4 (nn.Conv2d): Fourth convolutional layer.\n",
        "        bn4 (nn.BatchNorm2d): Batch normalization for the fourth convolutional layer.\n",
        "        fc1 (nn.Linear): First fully connected layer.\n",
        "        fc2 (nn.Linear): Second fully connected layer.\n",
        "        fc3 (nn.Linear): Output layer mapping to action values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_size, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the network.\n",
        "\n",
        "        Args:\n",
        "            action_size (int): Number of possible actions.\n",
        "            seed (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        super(Network, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(10 * 10 * 128, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): Input state (e.g., an image).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output action values.\n",
        "        \"\"\"\n",
        "        # Apply first convolutional layer with ReLU activation and batch normalization\n",
        "        x = F.relu(self.bn1(self.conv1(state)))\n",
        "\n",
        "        # Apply second convolutional layer with ReLU activation and batch normalization\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        # Apply third convolutional layer with ReLU activation and batch normalization\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Apply fourth convolutional layer with ReLU activation and batch normalization\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "        # Flatten the tensor for fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply first fully connected layer with ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply second fully connected layer with ReLU activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Output layer for action values\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUvCfE_mhwo2"
      },
      "source": [
        "## Part 2 - Training the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCDPF22lkwc"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2ifYmIc4vk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c39de2e-72a8-4740-aadd-e302fbe1406d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "State size:  210\n",
            "Number of actions:  9\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "# Create the Ms. Pac-Man environment with deterministic settings\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space=False)\n",
        "\n",
        "# Get the shape of the observation space (state)\n",
        "state_shape = env.observation_space.shape\n",
        "\n",
        "# Get the size of the state (the first dimension of the state shape, assuming a 3D state representation)\n",
        "state_size = env.observation_space.shape[0]\n",
        "\n",
        "# Get the number of possible actions in the environment\n",
        "number_actions = env.action_space.n\n",
        "\n",
        "# Print the shape of the state, the size of the state, and the number of possible actions\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 64\n",
        "discount_factor = 0.99"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBKMeuRy-_b-",
        "outputId": "a57d728f-db65-4a90-f410-9d511ec33373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2bDShIEkA5V"
      },
      "source": [
        "### Preprocessing the frames"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"\n",
        "    Preprocesses a given image frame by converting it to a PIL Image, applying transformations,\n",
        "    and adding a batch dimension.\n",
        "\n",
        "    Args:\n",
        "    frame (numpy.ndarray): The input image frame represented as a NumPy array.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The preprocessed image tensor with shape (1, C, H, W).\n",
        "                  - 1: Batch size\n",
        "                  - C: Number of channels (3 for RGB images, 1 for grayscale images)\n",
        "                  - H: Height of the image (128 pixels)\n",
        "                  - W: Width of the image (128 pixels)\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the input NumPy array to a PIL Image object\n",
        "    frame = Image.fromarray(frame)\n",
        "\n",
        "    # Define a series of transformations to be applied to the image\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),  # Resize the image to 128x128 pixels\n",
        "        transforms.ToTensor()           # Convert the image to a PyTorch tensor and normalize pixel values to [0, 1]\n",
        "    ])\n",
        "\n",
        "    # Apply the transformations to the image and add a batch dimension\n",
        "    # The unsqueeze(0) method adds a new dimension at position 0 (batch dimension)\n",
        "    return preprocess(frame).unsqueeze(0)"
      ],
      "metadata": {
        "id": "Re98YX2F_eHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imMdSO-HAWra"
      },
      "source": [
        "### Implementing the DCQN class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, action_size):\n",
        "        \"\"\"\n",
        "        Initializes the agent with the given action size and sets up the neural networks,\n",
        "        optimizer, and replay memory.\n",
        "\n",
        "        Args:\n",
        "        action_size (int): The number of possible actions the agent can take.\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.action_size = action_size\n",
        "        self.local_qnetwork = Network(action_size).to(self.device)\n",
        "        self.target_qnetwork = Network(action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Adds the experience to the replay memory and triggers learning if the memory is sufficiently large.\n",
        "\n",
        "        Args:\n",
        "        state (numpy.ndarray): The current state.\n",
        "        action (int): The action taken.\n",
        "        reward (float): The reward received.\n",
        "        next_state (numpy.ndarray): The next state.\n",
        "        done (bool): Whether the episode has ended.\n",
        "        \"\"\"\n",
        "        state = preprocess_frame(state)\n",
        "        next_state = preprocess_frame(next_state)\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > minibatch_size:\n",
        "            experiences = random.sample(self.memory, k=minibatch_size)\n",
        "            self.learn(experiences, discount_factor)\n",
        "\n",
        "    def act(self, state, epsilon=0.):\n",
        "        \"\"\"\n",
        "        Selects an action using an epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "        state (numpy.ndarray): The current state.\n",
        "        epsilon (float): The probability of selecting a random action (for exploration).\n",
        "\n",
        "        Returns:\n",
        "        int: The selected action.\n",
        "        \"\"\"\n",
        "        state = preprocess_frame(state).to(self.device)\n",
        "        self.local_qnetwork.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.local_qnetwork(state)\n",
        "        self.local_qnetwork.train()\n",
        "        if random.random() > epsilon:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, discount_factor):\n",
        "        \"\"\"\n",
        "        Updates the Q-network based on a batch of experience tuples.\n",
        "\n",
        "        Args:\n",
        "        experiences (list): A batch of experience tuples.\n",
        "        discount_factor (float): The discount factor (gamma).\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states = torch.cat(states).float().to(self.device)\n",
        "        actions = torch.tensor(actions).unsqueeze(1).long().to(self.device)\n",
        "        rewards = torch.tensor(rewards).unsqueeze(1).float().to(self.device)\n",
        "        next_states = torch.cat(next_states).float().to(self.device)\n",
        "        dones = torch.tensor(dones).unsqueeze(1).float().to(self.device)\n",
        "\n",
        "        # Compute the target Q-values\n",
        "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "\n",
        "        # Perform backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeaSC_CfT46t",
        "outputId": "ba7a61bb-40c8-4d4f-8770-299e5264369b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUg95iBpAwII"
      },
      "source": [
        "### Initializing the DCQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(number_actions)"
      ],
      "metadata": {
        "id": "CA-hexUWpIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      },
      "source": [
        "### Training the DCQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 10000\n",
        "epsilon_starting_value  = 1.0\n",
        "epsilon_ending_value  = 0.01\n",
        "epsilon_decay_value  = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "id": "CxzVVGT5pLct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0WhhBV8nQdf"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}